<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="es"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CrawlController.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">TFG</a> &gt; <a href="index.source.html" class="el_package">crawler</a> &gt; <span class="el_source">CrawlController.java</span></div><h1>CrawlController.java</h1><pre class="source lang-java linenums">/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the &quot;License&quot;); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package crawler;

import java.io.File;
import java.util.ArrayList;
import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;


/**
 * The controller that manages a crawling session. This class creates the
 * crawler threads and monitors their progress.
 *
 * @author Yasser Ganjisaffar
 */
public class CrawlController extends Configurable {

<span class="nc" id="L39">    static final Logger logger = LoggerFactory.getLogger(CrawlController.class);</span>

    /**
     * The 'customData' object can be used for passing custom crawl-related
     * configurations to different components of the crawler.
     */
    protected Object customData;

    /**
     * Once the crawling session finishes the controller collects the local data
     * of the crawler threads and stores them in this List.
     */
<span class="nc" id="L51">    protected List&lt;Object&gt; crawlersLocalData = new ArrayList&lt;&gt;();</span>

    /**
     * Is the crawling of this session finished?
     */
    protected boolean finished;

    /**
     * Is the crawling session set to 'shutdown'. Crawler threads monitor this
     * flag and when it is set they will no longer process new pages.
     */
    protected boolean shuttingDown;

    protected PageFetcher pageFetcher;
    protected RobotstxtServer robotstxtServer;
    protected Frontier frontier;
    protected DocIDServer docIdServer;

<span class="nc" id="L69">    protected final Object waitingLock = new Object();</span>
    protected final Environment env;

    public CrawlController(CrawlConfig config, PageFetcher pageFetcher,
                           RobotstxtServer robotstxtServer) throws Exception {
<span class="nc" id="L74">        super(config);</span>

<span class="nc" id="L76">        config.validate();</span>
<span class="nc" id="L77">        File folder = new File(config.getCrawlStorageFolder());</span>
<span class="nc bnc" id="L78" title="All 2 branches missed.">        if (!folder.exists()) {</span>
<span class="nc bnc" id="L79" title="All 2 branches missed.">            if (folder.mkdirs()) {</span>
<span class="nc" id="L80">                logger.debug(&quot;Created folder: &quot; + folder.getAbsolutePath());</span>
            } else {
<span class="nc" id="L82">                throw new Exception(</span>
<span class="nc" id="L83">                    &quot;couldn't create the storage folder: &quot; + folder.getAbsolutePath() +</span>
                    &quot; does it already exist ?&quot;);
            }
        }

<span class="nc" id="L88">        TLDList.setUseOnline(config.isOnlineTldListUpdate());</span>

<span class="nc" id="L90">        boolean resumable = config.isResumableCrawling();</span>

<span class="nc" id="L92">        EnvironmentConfig envConfig = new EnvironmentConfig();</span>
<span class="nc" id="L93">        envConfig.setAllowCreate(true);</span>
<span class="nc" id="L94">        envConfig.setTransactional(resumable);</span>
<span class="nc" id="L95">        envConfig.setLocking(resumable);</span>

<span class="nc" id="L97">        File envHome = new File(config.getCrawlStorageFolder() + &quot;/frontier&quot;);</span>
<span class="nc bnc" id="L98" title="All 2 branches missed.">        if (!envHome.exists()) {</span>
<span class="nc bnc" id="L99" title="All 2 branches missed.">            if (envHome.mkdir()) {</span>
<span class="nc" id="L100">                logger.debug(&quot;Created folder: &quot; + envHome.getAbsolutePath());</span>
            } else {
<span class="nc" id="L102">                throw new Exception(</span>
<span class="nc" id="L103">                    &quot;Failed creating the frontier folder: &quot; + envHome.getAbsolutePath());</span>
            }
        }

<span class="nc bnc" id="L107" title="All 2 branches missed.">        if (!resumable) {</span>
<span class="nc" id="L108">            IO.deleteFolderContents(envHome);</span>
<span class="nc" id="L109">            logger.info(&quot;Deleted contents of: &quot; + envHome +</span>
                        &quot; ( as you have configured resumable crawling to false )&quot;);
        }

<span class="nc" id="L113">        env = new Environment(envHome, envConfig);</span>
<span class="nc" id="L114">        docIdServer = new DocIDServer(env, config);</span>
<span class="nc" id="L115">        frontier = new Frontier(env, config);</span>

<span class="nc" id="L117">        this.pageFetcher = pageFetcher;</span>
<span class="nc" id="L118">        this.robotstxtServer = robotstxtServer;</span>

<span class="nc" id="L120">        finished = false;</span>
<span class="nc" id="L121">        shuttingDown = false;</span>
<span class="nc" id="L122">    }</span>

    public interface WebCrawlerFactory&lt;T extends WebCrawler&gt; {
        T newInstance() throws Exception;
    }

    private static class DefaultWebCrawlerFactory&lt;T extends WebCrawler&gt;
        implements WebCrawlerFactory&lt;T&gt; {
        final Class&lt;T&gt; clazz;

<span class="nc" id="L132">        DefaultWebCrawlerFactory(Class&lt;T&gt; clazz) {</span>
<span class="nc" id="L133">            this.clazz = clazz;</span>
<span class="nc" id="L134">        }</span>

        @Override
        public T newInstance() throws Exception {
            try {
<span class="nc" id="L139">                return clazz.newInstance();</span>
<span class="nc" id="L140">            } catch (ReflectiveOperationException e) {</span>
<span class="nc" id="L141">                throw e;</span>
            }
        }
    }

    /**
     * Start the crawling session and wait for it to finish.
     * This method utilizes default crawler factory that creates new crawler using Java reflection
     *
     * @param clazz
     *            the class that implements the logic for crawler threads
     * @param numberOfCrawlers
     *            the number of concurrent threads that will be contributing in
     *            this crawling session.
     * @param &lt;T&gt; Your class extending WebCrawler
     */
    public &lt;T extends WebCrawler&gt; void start(Class&lt;T&gt; clazz, int numberOfCrawlers) {
<span class="nc" id="L158">        this.start(new DefaultWebCrawlerFactory&lt;&gt;(clazz), numberOfCrawlers, true);</span>
<span class="nc" id="L159">    }</span>

    /**
     * Start the crawling session and wait for it to finish.
     *
     * @param crawlerFactory
     *            factory to create crawlers on demand for each thread
     * @param numberOfCrawlers
     *            the number of concurrent threads that will be contributing in
     *            this crawling session.
     * @param &lt;T&gt; Your class extending WebCrawler
     */
    public &lt;T extends WebCrawler&gt; void start(WebCrawlerFactory&lt;T&gt; crawlerFactory,
                                             int numberOfCrawlers) {
<span class="nc" id="L173">        this.start(crawlerFactory, numberOfCrawlers, true);</span>
<span class="nc" id="L174">    }</span>

    /**
     * Start the crawling session and return immediately.
     *
     * @param crawlerFactory
     *            factory to create crawlers on demand for each thread
     * @param numberOfCrawlers
     *            the number of concurrent threads that will be contributing in
     *            this crawling session.
     * @param &lt;T&gt; Your class extending WebCrawler
     */
    public &lt;T extends WebCrawler&gt; void startNonBlocking(WebCrawlerFactory&lt;T&gt; crawlerFactory,
                                                        final int numberOfCrawlers) {
<span class="nc" id="L188">        this.start(crawlerFactory, numberOfCrawlers, false);</span>
<span class="nc" id="L189">    }</span>

    /**
     * Start the crawling session and return immediately.
     * This method utilizes default crawler factory that creates new crawler using Java reflection
     *
     * @param clazz
     *            the class that implements the logic for crawler threads
     * @param numberOfCrawlers
     *            the number of concurrent threads that will be contributing in
     *            this crawling session.
     * @param &lt;T&gt; Your class extending WebCrawler
     */
    public &lt;T extends WebCrawler&gt; void startNonBlocking(Class&lt;T&gt; clazz, int numberOfCrawlers) {
<span class="nc" id="L203">        start(new DefaultWebCrawlerFactory&lt;&gt;(clazz), numberOfCrawlers, false);</span>
<span class="nc" id="L204">    }</span>

    protected &lt;T extends WebCrawler&gt; void start(final WebCrawlerFactory&lt;T&gt; crawlerFactory,
                                                final int numberOfCrawlers, boolean isBlocking) {
        try {
<span class="nc" id="L209">            finished = false;</span>
<span class="nc" id="L210">            crawlersLocalData.clear();</span>
<span class="nc" id="L211">            final List&lt;Thread&gt; threads = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L212">            final List&lt;T&gt; crawlers = new ArrayList&lt;&gt;();</span>

<span class="nc bnc" id="L214" title="All 2 branches missed.">            for (int i = 1; i &lt;= numberOfCrawlers; i++) {</span>
<span class="nc" id="L215">                T crawler = crawlerFactory.newInstance();</span>
<span class="nc" id="L216">                Thread thread = new Thread(crawler, &quot;Crawler &quot; + i);</span>
<span class="nc" id="L217">                crawler.setThread(thread);</span>
<span class="nc" id="L218">                crawler.init(i, this);</span>
<span class="nc" id="L219">                thread.start();</span>
<span class="nc" id="L220">                crawlers.add(crawler);</span>
<span class="nc" id="L221">                threads.add(thread);</span>
<span class="nc" id="L222">                logger.info(&quot;Crawler {} started&quot;, i);</span>
            }

<span class="nc" id="L225">            final CrawlController controller = this;</span>
<span class="nc" id="L226">            final CrawlConfig config = this.getConfig();</span>

<span class="nc" id="L228">            Thread monitorThread = new Thread(new Runnable() {</span>

                @Override
                public void run() {
                    try {
<span class="nc" id="L233">                        synchronized (waitingLock) {</span>

                            while (true) {
<span class="nc" id="L236">                                sleep(config.getThreadMonitoringDelaySeconds());</span>
<span class="nc" id="L237">                                boolean someoneIsWorking = false;</span>
<span class="nc bnc" id="L238" title="All 2 branches missed.">                                for (int i = 0; i &lt; threads.size(); i++) {</span>
<span class="nc" id="L239">                                    Thread thread = threads.get(i);</span>
<span class="nc bnc" id="L240" title="All 2 branches missed.">                                    if (!thread.isAlive()) {</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">                                        if (!shuttingDown) {</span>
<span class="nc" id="L242">                                            logger.info(&quot;Thread {} was dead, I'll recreate it&quot;, i);</span>
<span class="nc" id="L243">                                            T crawler = crawlerFactory.newInstance();</span>
<span class="nc" id="L244">                                            thread = new Thread(crawler, &quot;Crawler &quot; + (i + 1));</span>
<span class="nc" id="L245">                                            threads.remove(i);</span>
<span class="nc" id="L246">                                            threads.add(i, thread);</span>
<span class="nc" id="L247">                                            crawler.setThread(thread);</span>
<span class="nc" id="L248">                                            crawler.init(i + 1, controller);</span>
<span class="nc" id="L249">                                            thread.start();</span>
<span class="nc" id="L250">                                            crawlers.remove(i);</span>
<span class="nc" id="L251">                                            crawlers.add(i, crawler);</span>
<span class="nc" id="L252">                                        }</span>
<span class="nc bnc" id="L253" title="All 2 branches missed.">                                    } else if (crawlers.get(i).isNotWaitingForNewURLs()) {</span>
<span class="nc" id="L254">                                        someoneIsWorking = true;</span>
                                    }
                                }
<span class="nc" id="L257">                                boolean shutOnEmpty = config.isShutdownOnEmptyQueue();</span>
<span class="nc bnc" id="L258" title="All 4 branches missed.">                                if (!someoneIsWorking &amp;&amp; shutOnEmpty) {</span>
                                    // Make sure again that none of the threads
                                    // are
                                    // alive.
<span class="nc" id="L262">                                    logger.info(</span>
                                        &quot;It looks like no thread is working, waiting for &quot; +
<span class="nc" id="L264">                                         config.getThreadShutdownDelaySeconds() +</span>
                                         &quot; seconds to make sure...&quot;);
<span class="nc" id="L266">                                    sleep(config.getThreadShutdownDelaySeconds());</span>

<span class="nc" id="L268">                                    someoneIsWorking = false;</span>
<span class="nc bnc" id="L269" title="All 2 branches missed.">                                    for (int i = 0; i &lt; threads.size(); i++) {</span>
<span class="nc" id="L270">                                        Thread thread = threads.get(i);</span>
<span class="nc bnc" id="L271" title="All 2 branches missed.">                                        if (thread.isAlive() &amp;&amp;</span>
<span class="nc bnc" id="L272" title="All 2 branches missed.">                                            crawlers.get(i).isNotWaitingForNewURLs()) {</span>
<span class="nc" id="L273">                                            someoneIsWorking = true;</span>
                                        }
                                    }
<span class="nc bnc" id="L276" title="All 2 branches missed.">                                    if (!someoneIsWorking) {</span>
<span class="nc bnc" id="L277" title="All 2 branches missed.">                                        if (!shuttingDown) {</span>
<span class="nc" id="L278">                                            long queueLength = frontier.getQueueLength();</span>
<span class="nc bnc" id="L279" title="All 2 branches missed.">                                            if (queueLength &gt; 0) {</span>
<span class="nc" id="L280">                                                continue;</span>
                                            }
<span class="nc" id="L282">                                            logger.info(</span>
                                                &quot;No thread is working and no more URLs are in &quot; +
                                                &quot;queue waiting for another &quot; +
<span class="nc" id="L285">                                                config.getThreadShutdownDelaySeconds() +</span>
                                                &quot; seconds to make sure...&quot;);
<span class="nc" id="L287">                                            sleep(config.getThreadShutdownDelaySeconds());</span>
<span class="nc" id="L288">                                            queueLength = frontier.getQueueLength();</span>
<span class="nc bnc" id="L289" title="All 2 branches missed.">                                            if (queueLength &gt; 0) {</span>
<span class="nc" id="L290">                                                continue;</span>
                                            }
                                        }

<span class="nc" id="L294">                                        logger.info(</span>
                                            &quot;All of the crawlers are stopped. Finishing the &quot; +
                                            &quot;process...&quot;);
                                        // At this step, frontier notifies the threads that were
                                        // waiting for new URLs and they should stop
<span class="nc" id="L299">                                        frontier.finish();</span>
<span class="nc bnc" id="L300" title="All 2 branches missed.">                                        for (T crawler : crawlers) {</span>
<span class="nc" id="L301">                                            crawler.onBeforeExit();</span>
<span class="nc" id="L302">                                            crawlersLocalData.add(crawler.getMyLocalData());</span>
<span class="nc" id="L303">                                        }</span>

<span class="nc" id="L305">                                        logger.info(</span>
<span class="nc" id="L306">                                            &quot;Waiting for &quot; + config.getCleanupDelaySeconds() +</span>
                                            &quot; seconds before final clean up...&quot;);
<span class="nc" id="L308">                                        sleep(config.getCleanupDelaySeconds());</span>

<span class="nc" id="L310">                                        frontier.close();</span>
<span class="nc" id="L311">                                        docIdServer.close();</span>
<span class="nc" id="L312">                                        pageFetcher.shutDown();</span>

<span class="nc" id="L314">                                        finished = true;</span>
<span class="nc" id="L315">                                        waitingLock.notifyAll();</span>
<span class="nc" id="L316">                                        env.close();</span>

<span class="nc" id="L318">                                        return;</span>
                                    }
                                }
<span class="nc" id="L321">                            }</span>
<span class="nc" id="L322">                        }</span>
<span class="nc" id="L323">                    } catch (Exception e) {</span>
<span class="nc" id="L324">                        logger.error(&quot;Unexpected Error&quot;, e);</span>
                    }
<span class="nc" id="L326">                }</span>
            });

<span class="nc" id="L329">            monitorThread.start();</span>

<span class="nc bnc" id="L331" title="All 2 branches missed.">            if (isBlocking) {</span>
<span class="nc" id="L332">                waitUntilFinish();</span>
            }

<span class="nc" id="L335">        } catch (Exception e) {</span>
<span class="nc" id="L336">            logger.error(&quot;Error happened&quot;, e);</span>
<span class="nc" id="L337">        }</span>
<span class="nc" id="L338">    }</span>

    /**
     * Wait until this crawling session finishes.
     */
    public void waitUntilFinish() {
<span class="nc bnc" id="L344" title="All 2 branches missed.">        while (!finished) {</span>
<span class="nc" id="L345">            synchronized (waitingLock) {</span>
<span class="nc bnc" id="L346" title="All 2 branches missed.">                if (finished) {</span>
<span class="nc" id="L347">                    return;</span>
                }
                try {
<span class="nc" id="L350">                    waitingLock.wait();</span>
<span class="nc" id="L351">                } catch (InterruptedException e) {</span>
<span class="nc" id="L352">                    logger.error(&quot;Error occurred&quot;, e);</span>
<span class="nc" id="L353">                }</span>
<span class="nc" id="L354">            }</span>
        }
<span class="nc" id="L356">    }</span>

    /**
     * Once the crawling session finishes the controller collects the local data of the crawler
     * threads and stores them
     * in a List.
     * This function returns the reference to this list.
     *
     * @return List of Objects which are your local data
     */
    public List&lt;Object&gt; getCrawlersLocalData() {
<span class="nc" id="L367">        return crawlersLocalData;</span>
    }

    protected static void sleep(int seconds) {
        try {
<span class="nc" id="L372">            Thread.sleep(seconds * 1000);</span>
<span class="nc" id="L373">        } catch (InterruptedException ignored) {</span>
            // Do nothing
<span class="nc" id="L375">        }</span>
<span class="nc" id="L376">    }</span>

    /**
     * Adds a new seed URL. A seed URL is a URL that is fetched by the crawler
     * to extract new URLs in it and follow them for crawling.
     *
     * @param pageUrl
     *            the URL of the seed
     */
    public void addSeed(String pageUrl) {
<span class="nc" id="L386">        addSeed(pageUrl, -1);</span>
<span class="nc" id="L387">    }</span>

    /**
     * Adds a new seed URL. A seed URL is a URL that is fetched by the crawler
     * to extract new URLs in it and follow them for crawling. You can also
     * specify a specific document id to be assigned to this seed URL. This
     * document id needs to be unique. Also, note that if you add three seeds
     * with document ids 1,2, and 7. Then the next URL that is found during the
     * crawl will get a doc id of 8. Also you need to ensure to add seeds in
     * increasing order of document ids.
     *
     * Specifying doc ids is mainly useful when you have had a previous crawl
     * and have stored the results and want to start a new crawl with seeds
     * which get the same document ids as the previous crawl.
     *
     * @param pageUrl
     *            the URL of the seed
     * @param docId
     *            the document id that you want to be assigned to this seed URL.
     *
     */
    public void addSeed(String pageUrl, int docId) {
<span class="nc" id="L409">        String canonicalUrl = URLCanonicalizer.getCanonicalURL(pageUrl);</span>
<span class="nc bnc" id="L410" title="All 2 branches missed.">        if (canonicalUrl == null) {</span>
<span class="nc" id="L411">            logger.error(&quot;Invalid seed URL: {}&quot;, pageUrl);</span>
        } else {
<span class="nc bnc" id="L413" title="All 2 branches missed.">            if (docId &lt; 0) {</span>
<span class="nc" id="L414">                docId = docIdServer.getDocId(canonicalUrl);</span>
<span class="nc bnc" id="L415" title="All 2 branches missed.">                if (docId &gt; 0) {</span>
<span class="nc" id="L416">                    logger.trace(&quot;This URL is already seen.&quot;);</span>
<span class="nc" id="L417">                    return;</span>
                }
<span class="nc" id="L419">                docId = docIdServer.getNewDocID(canonicalUrl);</span>
            } else {
                try {
<span class="nc" id="L422">                    docIdServer.addUrlAndDocId(canonicalUrl, docId);</span>
<span class="nc" id="L423">                } catch (Exception e) {</span>
<span class="nc" id="L424">                    logger.error(&quot;Could not add seed: {}&quot;, e.getMessage());</span>
<span class="nc" id="L425">                }</span>
            }

<span class="nc" id="L428">            WebURL webUrl = new WebURL();</span>
<span class="nc" id="L429">            webUrl.setURL(canonicalUrl);</span>
<span class="nc" id="L430">            webUrl.setDocid(docId);</span>
<span class="nc" id="L431">            webUrl.setDepth((short) 0);</span>
<span class="nc bnc" id="L432" title="All 2 branches missed.">            if (robotstxtServer.allows(webUrl)) {</span>
<span class="nc" id="L433">                frontier.schedule(webUrl);</span>
            } else {
                // using the WARN level here, as the user specifically asked to add this seed
<span class="nc" id="L436">                logger.warn(&quot;Robots.txt does not allow this seed: {}&quot;, pageUrl);</span>
            }
        }
<span class="nc" id="L439">    }</span>

    /**
     * This function can called to assign a specific document id to a url. This
     * feature is useful when you have had a previous crawl and have stored the
     * Urls and their associated document ids and want to have a new crawl which
     * is aware of the previously seen Urls and won't re-crawl them.
     *
     * Note that if you add three seen Urls with document ids 1,2, and 7. Then
     * the next URL that is found during the crawl will get a doc id of 8. Also
     * you need to ensure to add seen Urls in increasing order of document ids.
     *
     * @param url
     *            the URL of the page
     * @param docId
     *            the document id that you want to be assigned to this URL.
     *
     */
    public void addSeenUrl(String url, int docId) {
<span class="nc" id="L458">        String canonicalUrl = URLCanonicalizer.getCanonicalURL(url);</span>
<span class="nc bnc" id="L459" title="All 2 branches missed.">        if (canonicalUrl == null) {</span>
<span class="nc" id="L460">            logger.error(&quot;Invalid Url: {} (can't cannonicalize it!)&quot;, url);</span>
        } else {
            try {
<span class="nc" id="L463">                docIdServer.addUrlAndDocId(canonicalUrl, docId);</span>
<span class="nc" id="L464">            } catch (Exception e) {</span>
<span class="nc" id="L465">                logger.error(&quot;Could not add seen url: {}&quot;, e.getMessage());</span>
<span class="nc" id="L466">            }</span>
        }
<span class="nc" id="L468">    }</span>

    public PageFetcher getPageFetcher() {
<span class="nc" id="L471">        return pageFetcher;</span>
    }

    public void setPageFetcher(PageFetcher pageFetcher) {
<span class="nc" id="L475">        this.pageFetcher = pageFetcher;</span>
<span class="nc" id="L476">    }</span>

    public RobotstxtServer getRobotstxtServer() {
<span class="nc" id="L479">        return robotstxtServer;</span>
    }

    public void setRobotstxtServer(RobotstxtServer robotstxtServer) {
<span class="nc" id="L483">        this.robotstxtServer = robotstxtServer;</span>
<span class="nc" id="L484">    }</span>

    public Frontier getFrontier() {
<span class="nc" id="L487">        return frontier;</span>
    }

    public void setFrontier(Frontier frontier) {
<span class="nc" id="L491">        this.frontier = frontier;</span>
<span class="nc" id="L492">    }</span>

    public DocIDServer getDocIdServer() {
<span class="nc" id="L495">        return docIdServer;</span>
    }

    public void setDocIdServer(DocIDServer docIdServer) {
<span class="nc" id="L499">        this.docIdServer = docIdServer;</span>
<span class="nc" id="L500">    }</span>

    public Object getCustomData() {
<span class="nc" id="L503">        return customData;</span>
    }

    public void setCustomData(Object customData) {
<span class="nc" id="L507">        this.customData = customData;</span>
<span class="nc" id="L508">    }</span>

    public boolean isFinished() {
<span class="nc" id="L511">        return this.finished;</span>
    }

    public boolean isShuttingDown() {
<span class="nc" id="L515">        return shuttingDown;</span>
    }

    /**
     * Set the current crawling session set to 'shutdown'. Crawler threads
     * monitor the shutdown flag and when it is set to true, they will no longer
     * process new pages.
     */
    public void shutdown() {
<span class="nc" id="L524">        logger.info(&quot;Shutting down...&quot;);</span>
<span class="nc" id="L525">        this.shuttingDown = true;</span>
<span class="nc" id="L526">        pageFetcher.shutDown();</span>
<span class="nc" id="L527">        frontier.finish();</span>
<span class="nc" id="L528">    }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.9.201702052155</span></div></body></html>