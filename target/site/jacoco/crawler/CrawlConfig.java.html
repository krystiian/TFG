<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="es"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CrawlConfig.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">TFG</a> &gt; <a href="index.source.html" class="el_package">crawler</a> &gt; <span class="el_source">CrawlConfig.java</span></div><h1>CrawlConfig.java</h1><pre class="source lang-java linenums">/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the &quot;License&quot;); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package crawler;

import java.util.ArrayList;
import java.util.Collection;
import java.util.HashSet;
import java.util.List;

import org.apache.http.Header;
import org.apache.http.client.config.CookieSpecs;
import org.apache.http.message.BasicHeader;


<span class="nc" id="L30">public class CrawlConfig {</span>

    /**
     * The folder which will be used by crawler for storing the intermediate
     * crawl data. The content of this folder should not be modified manually.
     */
    private String crawlStorageFolder;

    /**
     * If this feature is enabled, you would be able to resume a previously
     * stopped/crashed crawl. However, it makes crawling slightly slower
     */
<span class="nc" id="L42">    private boolean resumableCrawling = false;</span>

    /**
     * Maximum depth of crawling For unlimited depth this parameter should be
     * set to -1
     */
<span class="nc" id="L48">    private int maxDepthOfCrawling = -1;</span>

    /**
     * Maximum number of pages to fetch For unlimited number of pages, this
     * parameter should be set to -1
     */
<span class="nc" id="L54">    private int maxPagesToFetch = -1;</span>

    /**
     * user-agent string that is used for representing your crawler to web
     * servers. See http://en.wikipedia.org/wiki/User_agent for more details
     */
<span class="nc" id="L60">    private String userAgentString = &quot;crawler4j (https://github.com/yasserg/crawler4j/)&quot;;</span>

    /**
     * Default request header values.
     */
<span class="nc" id="L65">    private Collection&lt;BasicHeader&gt; defaultHeaders = new HashSet&lt;BasicHeader&gt;();</span>

    /**
     * Politeness delay in milliseconds (delay between sending two requests to
     * the same host).
     */
<span class="nc" id="L71">    private int politenessDelay = 200;</span>

    /**
     * Should we also crawl https pages?
     */
<span class="nc" id="L76">    private boolean includeHttpsPages = true;</span>

    /**
     * Should we fetch binary content such as images, audio, ...?
     */
<span class="nc" id="L81">    private boolean includeBinaryContentInCrawling = false;</span>

    /**
     * Should we process binary content such as image, audio, ... using TIKA?
     */
<span class="nc" id="L86">    private boolean processBinaryContentInCrawling = false;</span>

    /**
     * Maximum Connections per host
     */
<span class="nc" id="L91">    private int maxConnectionsPerHost = 100;</span>

    /**
     * Maximum total connections
     */
<span class="nc" id="L96">    private int maxTotalConnections = 100;</span>

    /**
     * Socket timeout in milliseconds
     */
<span class="nc" id="L101">    private int socketTimeout = 20000;</span>

    /**
     * Connection timeout in milliseconds
     */
<span class="nc" id="L106">    private int connectionTimeout = 30000;</span>

    /**
     * Max number of outgoing links which are processed from a page
     */
<span class="nc" id="L111">    private int maxOutgoingLinksToFollow = 5000;</span>

    /**
     * Max allowed size of a page. Pages larger than this size will not be
     * fetched.
     */
<span class="nc" id="L117">    private int maxDownloadSize = 1048576;</span>

    /**
     * Should we follow redirects?
     */
<span class="nc" id="L122">    private boolean followRedirects = true;</span>

    /**
     * Should the TLD list be updated automatically on each run? Alternatively,
     * it can be loaded from the embedded tld-names.zip file that was obtained from
     * https://publicsuffix.org/list/effective_tld_names.dat
     */
<span class="nc" id="L129">    private boolean onlineTldListUpdate = false;</span>

    /**
     * Should the crawler stop running when the queue is empty?
     */
<span class="nc" id="L134">    private boolean shutdownOnEmptyQueue = true;</span>

    /**
     * Wait this long before checking the status of the worker threads.
     */
<span class="nc" id="L139">    private int threadMonitoringDelaySeconds = 10;</span>

    /**
     * Wait this long to verify the craweler threads are finished working.
     */
<span class="nc" id="L144">    private int threadShutdownDelaySeconds = 10;</span>

    /**
     * Wait this long in seconds before launching cleanup.
     */
<span class="nc" id="L149">    private int cleanupDelaySeconds = 10;</span>

    /**
     * If crawler should run behind a proxy, this parameter can be used for
     * specifying the proxy host.
     */
<span class="nc" id="L155">    private String proxyHost = null;</span>

    /**
     * If crawler should run behind a proxy, this parameter can be used for
     * specifying the proxy port.
     */
<span class="nc" id="L161">    private int proxyPort = 80;</span>

    /**
     * If crawler should run behind a proxy and user/pass is needed for
     * authentication in proxy, this parameter can be used for specifying the
     * username.
     */
<span class="nc" id="L168">    private String proxyUsername = null;</span>

    /**
     * If crawler should run behind a proxy and user/pass is needed for
     * authentication in proxy, this parameter can be used for specifying the
     * password.
     */
<span class="nc" id="L175">    private String proxyPassword = null;</span>

    /**
     * List of possible authentications needed by crawler
     */
    private List&lt;AuthInfo&gt; authInfos;

    /**
     * Cookie policy
     */
<span class="nc" id="L185">    private String cookiePolicy = CookieSpecs.STANDARD;</span>

    /**
     * Whether to honor &quot;nofollow&quot; flag
     */
<span class="nc" id="L190">    private boolean respectNoFollow = true;</span>

    /**
     * Whether to honor &quot;noindex&quot; flag
     */
<span class="nc" id="L195">    private boolean respectNoIndex = true;</span>

    /**
     * Validates the configs specified by this instance.
     *
     * @throws Exception on Validation fail
     */
    public void validate() throws Exception {
<span class="nc bnc" id="L203" title="All 2 branches missed.">        if (crawlStorageFolder == null) {</span>
<span class="nc" id="L204">            throw new Exception(&quot;Crawl storage folder is not set in the CrawlConfig.&quot;);</span>
        }
<span class="nc bnc" id="L206" title="All 2 branches missed.">        if (politenessDelay &lt; 0) {</span>
<span class="nc" id="L207">            throw new Exception(&quot;Invalid value for politeness delay: &quot; + politenessDelay);</span>
        }
<span class="nc bnc" id="L209" title="All 2 branches missed.">        if (maxDepthOfCrawling &lt; -1) {</span>
<span class="nc" id="L210">            throw new Exception(</span>
                &quot;Maximum crawl depth should be either a positive number or -1 for unlimited depth&quot; +
                &quot;.&quot;);
        }
<span class="nc bnc" id="L214" title="All 2 branches missed.">        if (maxDepthOfCrawling &gt; Short.MAX_VALUE) {</span>
<span class="nc" id="L215">            throw new Exception(&quot;Maximum value for crawl depth is &quot; + Short.MAX_VALUE);</span>
        }
<span class="nc" id="L217">    }</span>

    public String getCrawlStorageFolder() {
<span class="nc" id="L220">        return crawlStorageFolder;</span>
    }

    /**
     * The folder which will be used by crawler for storing the intermediate
     * crawl data. The content of this folder should not be modified manually.
     *
     * @param crawlStorageFolder The folder for the crawler's storage
     */
    public void setCrawlStorageFolder(String crawlStorageFolder) {
<span class="nc" id="L230">        this.crawlStorageFolder = crawlStorageFolder;</span>
<span class="nc" id="L231">    }</span>

    public boolean isResumableCrawling() {
<span class="nc" id="L234">        return resumableCrawling;</span>
    }

    /**
     * If this feature is enabled, you would be able to resume a previously
     * stopped/crashed crawl. However, it makes crawling slightly slower
     *
     * @param resumableCrawling Should crawling be resumable between runs ?
     */
    public void setResumableCrawling(boolean resumableCrawling) {
<span class="nc" id="L244">        this.resumableCrawling = resumableCrawling;</span>
<span class="nc" id="L245">    }</span>

    public int getMaxDepthOfCrawling() {
<span class="nc" id="L248">        return maxDepthOfCrawling;</span>
    }

    /**
     * Maximum depth of crawling For unlimited depth this parameter should be set to -1
     *
     * @param maxDepthOfCrawling Depth of crawling (all links on current page = depth of 1)
     */
    public void setMaxDepthOfCrawling(int maxDepthOfCrawling) {
<span class="nc" id="L257">        this.maxDepthOfCrawling = maxDepthOfCrawling;</span>
<span class="nc" id="L258">    }</span>

    public int getMaxPagesToFetch() {
<span class="nc" id="L261">        return maxPagesToFetch;</span>
    }

    /**
     * Maximum number of pages to fetch For unlimited number of pages, this parameter should be
     * set to -1
     *
     * @param maxPagesToFetch How many pages to fetch from all threads together ?
     */
    public void setMaxPagesToFetch(int maxPagesToFetch) {
<span class="nc" id="L271">        this.maxPagesToFetch = maxPagesToFetch;</span>
<span class="nc" id="L272">    }</span>

    /**
     *
     * @return userAgentString
     */
    public String getUserAgentString() {
<span class="nc" id="L279">        return userAgentString;</span>
    }

    /**
     * user-agent string that is used for representing your crawler to web
     * servers. See http://en.wikipedia.org/wiki/User_agent for more details
     *
     * @param userAgentString Custom userAgent string to use as your crawler's identifier
     */
    public void setUserAgentString(String userAgentString) {
<span class="nc" id="L289">        this.userAgentString = userAgentString;</span>
<span class="nc" id="L290">    }</span>

    /**
     * Return a copy of the default header collection.
     */
    public Collection&lt;BasicHeader&gt; getDefaultHeaders() {
<span class="nc" id="L296">        return new HashSet&lt;&gt;(defaultHeaders);</span>
    }

    /**
     * Set the default header collection (creating copies of the provided headers).
     */
    public void setDefaultHeaders(Collection&lt;? extends Header&gt; defaultHeaders) {
<span class="nc" id="L303">        Collection&lt;BasicHeader&gt; copiedHeaders = new HashSet&lt;&gt;();</span>
<span class="nc bnc" id="L304" title="All 2 branches missed.">        for (Header header : defaultHeaders) {</span>
<span class="nc" id="L305">            copiedHeaders.add(new BasicHeader(header.getName(), header.getValue()));</span>
<span class="nc" id="L306">        }</span>
<span class="nc" id="L307">        this.defaultHeaders = copiedHeaders;</span>
<span class="nc" id="L308">    }</span>

    public int getPolitenessDelay() {
<span class="nc" id="L311">        return politenessDelay;</span>
    }

    /**
     * Politeness delay in milliseconds (delay between sending two requests to
     * the same host).
     *
     * @param politenessDelay
     *            the delay in milliseconds.
     */
    public void setPolitenessDelay(int politenessDelay) {
<span class="nc" id="L322">        this.politenessDelay = politenessDelay;</span>
<span class="nc" id="L323">    }</span>

    public boolean isIncludeHttpsPages() {
<span class="nc" id="L326">        return includeHttpsPages;</span>
    }

    /**
     * @param includeHttpsPages Should we crawl https pages?
     */
    public void setIncludeHttpsPages(boolean includeHttpsPages) {
<span class="nc" id="L333">        this.includeHttpsPages = includeHttpsPages;</span>
<span class="nc" id="L334">    }</span>

    public boolean isIncludeBinaryContentInCrawling() {
<span class="nc" id="L337">        return includeBinaryContentInCrawling;</span>
    }

    /**
     *
     * @param includeBinaryContentInCrawling Should we fetch binary content such as images,
     * audio, ...?
     */
    public void setIncludeBinaryContentInCrawling(boolean includeBinaryContentInCrawling) {
<span class="nc" id="L346">        this.includeBinaryContentInCrawling = includeBinaryContentInCrawling;</span>
<span class="nc" id="L347">    }</span>

    public boolean isProcessBinaryContentInCrawling() {
<span class="nc" id="L350">        return processBinaryContentInCrawling;</span>
    }

    /**
     * Should we process binary content such as images, audio, ... using TIKA?
     */
    public void setProcessBinaryContentInCrawling(boolean processBinaryContentInCrawling) {
<span class="nc" id="L357">        this.processBinaryContentInCrawling = processBinaryContentInCrawling;</span>
<span class="nc" id="L358">    }</span>

    public int getMaxConnectionsPerHost() {
<span class="nc" id="L361">        return maxConnectionsPerHost;</span>
    }

    /**
     * @param maxConnectionsPerHost Maximum Connections per host
     */
    public void setMaxConnectionsPerHost(int maxConnectionsPerHost) {
<span class="nc" id="L368">        this.maxConnectionsPerHost = maxConnectionsPerHost;</span>
<span class="nc" id="L369">    }</span>

    public int getMaxTotalConnections() {
<span class="nc" id="L372">        return maxTotalConnections;</span>
    }

    /**
     * @param maxTotalConnections Maximum total connections
     */
    public void setMaxTotalConnections(int maxTotalConnections) {
<span class="nc" id="L379">        this.maxTotalConnections = maxTotalConnections;</span>
<span class="nc" id="L380">    }</span>

    public int getSocketTimeout() {
<span class="nc" id="L383">        return socketTimeout;</span>
    }

    /**
     * @param socketTimeout Socket timeout in milliseconds
     */
    public void setSocketTimeout(int socketTimeout) {
<span class="nc" id="L390">        this.socketTimeout = socketTimeout;</span>
<span class="nc" id="L391">    }</span>

    public int getConnectionTimeout() {
<span class="nc" id="L394">        return connectionTimeout;</span>
    }

    /**
     * @param connectionTimeout Connection timeout in milliseconds
     */
    public void setConnectionTimeout(int connectionTimeout) {
<span class="nc" id="L401">        this.connectionTimeout = connectionTimeout;</span>
<span class="nc" id="L402">    }</span>

    public int getMaxOutgoingLinksToFollow() {
<span class="nc" id="L405">        return maxOutgoingLinksToFollow;</span>
    }

    /**
     * @param maxOutgoingLinksToFollow Max number of outgoing links which are processed from a page
     */
    public void setMaxOutgoingLinksToFollow(int maxOutgoingLinksToFollow) {
<span class="nc" id="L412">        this.maxOutgoingLinksToFollow = maxOutgoingLinksToFollow;</span>
<span class="nc" id="L413">    }</span>

    public int getMaxDownloadSize() {
<span class="nc" id="L416">        return maxDownloadSize;</span>
    }

    /**
     * @param maxDownloadSize Max allowed size of a page. Pages larger than this size will not be
     * fetched.
     */
    public void setMaxDownloadSize(int maxDownloadSize) {
<span class="nc" id="L424">        this.maxDownloadSize = maxDownloadSize;</span>
<span class="nc" id="L425">    }</span>

    public boolean isFollowRedirects() {
<span class="nc" id="L428">        return followRedirects;</span>
    }

    /**
     * @param followRedirects Should we follow redirects?
     */
    public void setFollowRedirects(boolean followRedirects) {
<span class="nc" id="L435">        this.followRedirects = followRedirects;</span>
<span class="nc" id="L436">    }</span>

    public boolean isShutdownOnEmptyQueue() {
<span class="nc" id="L439">        return shutdownOnEmptyQueue;</span>
    }

    /**
     * Should the crawler stop running when the queue is empty?
     */
    public void setShutdownOnEmptyQueue(boolean shutdown) {
<span class="nc" id="L446">        shutdownOnEmptyQueue = shutdown;</span>
<span class="nc" id="L447">    }</span>

    public boolean isOnlineTldListUpdate() {
<span class="nc" id="L450">        return onlineTldListUpdate;</span>
    }

    /**
     * Should the TLD list be updated automatically on each run? Alternatively,
     * it can be loaded from the embedded tld-names.txt resource file that was
     * obtained from https://publicsuffix.org/list/effective_tld_names.dat
     */
    public void setOnlineTldListUpdate(boolean online) {
<span class="nc" id="L459">        onlineTldListUpdate = online;</span>
<span class="nc" id="L460">    }</span>

    public String getProxyHost() {
<span class="nc" id="L463">        return proxyHost;</span>
    }

    /**
     * @param proxyHost If crawler should run behind a proxy, this parameter can be used for
     * specifying the proxy host.
     */
    public void setProxyHost(String proxyHost) {
<span class="nc" id="L471">        this.proxyHost = proxyHost;</span>
<span class="nc" id="L472">    }</span>

    public int getProxyPort() {
<span class="nc" id="L475">        return proxyPort;</span>
    }

    /**
     * @param proxyPort If crawler should run behind a proxy, this parameter can be used for
     * specifying the proxy port.
     */
    public void setProxyPort(int proxyPort) {
<span class="nc" id="L483">        this.proxyPort = proxyPort;</span>
<span class="nc" id="L484">    }</span>

    public String getProxyUsername() {
<span class="nc" id="L487">        return proxyUsername;</span>
    }

    /**
     * @param proxyUsername
     *        If crawler should run behind a proxy and user/pass is needed for
     *        authentication in proxy, this parameter can be used for specifying the username.
     */
    public void setProxyUsername(String proxyUsername) {
<span class="nc" id="L496">        this.proxyUsername = proxyUsername;</span>
<span class="nc" id="L497">    }</span>

    public String getProxyPassword() {
<span class="nc" id="L500">        return proxyPassword;</span>
    }

    /**
     * If crawler should run behind a proxy and user/pass is needed for
     * authentication in proxy, this parameter can be used for specifying the password.
     *
     * @param proxyPassword String Password
     */
    public void setProxyPassword(String proxyPassword) {
<span class="nc" id="L510">        this.proxyPassword = proxyPassword;</span>
<span class="nc" id="L511">    }</span>

    /**
     * @return the authentications Information
     */
    public List&lt;AuthInfo&gt; getAuthInfos() {
<span class="nc" id="L517">        return authInfos;</span>
    }

    public void addAuthInfo(AuthInfo authInfo) {
<span class="nc bnc" id="L521" title="All 2 branches missed.">        if (this.authInfos == null) {</span>
<span class="nc" id="L522">            this.authInfos = new ArrayList&lt;AuthInfo&gt;();</span>
        }

<span class="nc" id="L525">        this.authInfos.add(authInfo);</span>
<span class="nc" id="L526">    }</span>

    /**
     * @param authInfos authenticationInformations to set
     */
    public void setAuthInfos(List&lt;AuthInfo&gt; authInfos) {
<span class="nc" id="L532">        this.authInfos = authInfos;</span>
<span class="nc" id="L533">    }</span>

    public int getThreadMonitoringDelaySeconds() {
<span class="nc" id="L536">        return threadMonitoringDelaySeconds;</span>
    }

    public void setThreadMonitoringDelaySeconds(int delay) {
<span class="nc" id="L540">        this.threadMonitoringDelaySeconds = delay;</span>
<span class="nc" id="L541">    }</span>

    public int getThreadShutdownDelaySeconds() {
<span class="nc" id="L544">        return threadShutdownDelaySeconds;</span>
    }

    public void setThreadShutdownDelaySeconds(int delay) {
<span class="nc" id="L548">        this.threadShutdownDelaySeconds = delay;</span>
<span class="nc" id="L549">    }</span>

    public int getCleanupDelaySeconds() {
<span class="nc" id="L552">        return cleanupDelaySeconds;</span>
    }

    public void setCleanupDelaySeconds(int delay) {
<span class="nc" id="L556">        this.cleanupDelaySeconds = delay;</span>
<span class="nc" id="L557">    }</span>

    public String getCookiePolicy() {
<span class="nc" id="L560">        return cookiePolicy;</span>
    }

    public void setCookiePolicy(String cookiePolicy) {
<span class="nc" id="L564">        this.cookiePolicy = cookiePolicy;</span>
<span class="nc" id="L565">    }</span>

    public boolean isRespectNoFollow() {
<span class="nc" id="L568">        return respectNoFollow;</span>
    }

    public void setRespectNoFollow(boolean respectNoFollow) {
<span class="nc" id="L572">        this.respectNoFollow = respectNoFollow;</span>
<span class="nc" id="L573">    }</span>

    public boolean isRespectNoIndex() {
<span class="nc" id="L576">        return respectNoIndex;</span>
    }

    public void setRespectNoIndex(boolean respectNoIndex) {
<span class="nc" id="L580">        this.respectNoIndex = respectNoIndex;</span>
<span class="nc" id="L581">    }</span>

    @Override
    public String toString() {
<span class="nc" id="L585">        StringBuilder sb = new StringBuilder();</span>
<span class="nc" id="L586">        sb.append(&quot;Crawl storage folder: &quot; + getCrawlStorageFolder() + &quot;\n&quot;);</span>
<span class="nc" id="L587">        sb.append(&quot;Resumable crawling: &quot; + isResumableCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L588">        sb.append(&quot;Max depth of crawl: &quot; + getMaxDepthOfCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L589">        sb.append(&quot;Max pages to fetch: &quot; + getMaxPagesToFetch() + &quot;\n&quot;);</span>
<span class="nc" id="L590">        sb.append(&quot;User agent string: &quot; + getUserAgentString() + &quot;\n&quot;);</span>
<span class="nc" id="L591">        sb.append(&quot;Include https pages: &quot; + isIncludeHttpsPages() + &quot;\n&quot;);</span>
<span class="nc" id="L592">        sb.append(&quot;Include binary content: &quot; + isIncludeBinaryContentInCrawling() + &quot;\n&quot;);</span>
<span class="nc" id="L593">        sb.append(&quot;Max connections per host: &quot; + getMaxConnectionsPerHost() + &quot;\n&quot;);</span>
<span class="nc" id="L594">        sb.append(&quot;Max total connections: &quot; + getMaxTotalConnections() + &quot;\n&quot;);</span>
<span class="nc" id="L595">        sb.append(&quot;Socket timeout: &quot; + getSocketTimeout() + &quot;\n&quot;);</span>
<span class="nc" id="L596">        sb.append(&quot;Max total connections: &quot; + getMaxTotalConnections() + &quot;\n&quot;);</span>
<span class="nc" id="L597">        sb.append(&quot;Max outgoing links to follow: &quot; + getMaxOutgoingLinksToFollow() + &quot;\n&quot;);</span>
<span class="nc" id="L598">        sb.append(&quot;Max download size: &quot; + getMaxDownloadSize() + &quot;\n&quot;);</span>
<span class="nc" id="L599">        sb.append(&quot;Should follow redirects?: &quot; + isFollowRedirects() + &quot;\n&quot;);</span>
<span class="nc" id="L600">        sb.append(&quot;Proxy host: &quot; + getProxyHost() + &quot;\n&quot;);</span>
<span class="nc" id="L601">        sb.append(&quot;Proxy port: &quot; + getProxyPort() + &quot;\n&quot;);</span>
<span class="nc" id="L602">        sb.append(&quot;Proxy username: &quot; + getProxyUsername() + &quot;\n&quot;);</span>
<span class="nc" id="L603">        sb.append(&quot;Proxy password: &quot; + getProxyPassword() + &quot;\n&quot;);</span>
<span class="nc" id="L604">        sb.append(&quot;Thread monitoring delay: &quot; + getThreadMonitoringDelaySeconds() + &quot;\n&quot;);</span>
<span class="nc" id="L605">        sb.append(&quot;Thread shutdown delay: &quot; + getThreadShutdownDelaySeconds() + &quot;\n&quot;);</span>
<span class="nc" id="L606">        sb.append(&quot;Cleanup delay: &quot; + getCleanupDelaySeconds() + &quot;\n&quot;);</span>
<span class="nc" id="L607">        sb.append(&quot;Cookie policy: &quot; + getCookiePolicy() + &quot;\n&quot;);</span>
<span class="nc" id="L608">        sb.append(&quot;Respect nofollow: &quot; + isRespectNoFollow() + &quot;\n&quot;);</span>
<span class="nc" id="L609">        sb.append(&quot;Respect noindex: &quot; + isRespectNoIndex() + &quot;\n&quot;);</span>
<span class="nc" id="L610">        return sb.toString();</span>
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.9.201702052155</span></div></body></html>